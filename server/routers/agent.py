import asyncio
import base64
import subprocess
import json
import os
from pathlib import Path
import traceback
from typing import Optional
from fastapi import APIRouter, Request, WebSocket, Query, HTTPException
from fastapi.responses import FileResponse
import asyncio
import aiohttp
import requests
from services.agent_service import openai_client, anthropic_client, ollama_client
from services.mcp import MCPClient
from services.config_service import config_service, app_config, USER_DATA_DIR
from starlette.websockets import WebSocketDisconnect
from services.db_service import db_service
from routers.image_tools import generate_image, generate_image_tool
from routers.websocket import active_websockets, send_to_websocket

llm_config = config_service.get_config()

wsrouter = APIRouter()
stream_tasks = {}

@wsrouter.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, session_id: str = Query(...)):
    await websocket.accept()
    active_websockets[session_id] = websocket
    try:
        # Keep the connection alive
        while True:
            # Wait for messages (optional, if you need to receive from client)
            data = await websocket.receive_text()
            # Process the message if needed
    except WebSocketDisconnect as e:
        print(f"WebSocket disconnected: {e.code}, {e.reason}")
    except Exception as e:
        print(f"WebSocket error: {e}")
        traceback.print_exc()
    finally:
        if session_id in active_websockets:
            del active_websockets[session_id]

async def finish_chat(args_json: dict, ctx: dict):
    return []

# class ToolCall:
#     def __init__(self, id: str, name: str, arguments: str):
#         self.id = id
#         self.name = name
#         self.arguments = arguments

SYSTEM_TOOLS_MAPPING = {
    'finish': finish_chat,
    'generate_image': generate_image
}
SYSTEM_TOOLS = [
        {
            "type": "function",
            "function": {
                "name": "finish",
                "description": "You MUST call this tool when you think the task is finished or you think you can't do anything more. Otherwise, you will be continuously asked to do more about this task indefinitely. Calling this tool will end your turn on this task and hand it over to the user for further instructions.",
                "parameters": {
                    "type": "object",
                    "properties": {
                    }
                },
            }
        },
        {
            "type": "function",
            "function": {
                "name": "generate_image",
                "description": "Generate an image using text prompt or optionally pass an image for reference or for editing",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "prompt": {
                            "type": "string",
                            "description": "Required. The prompt for image generation. If you want to edit an image, please describe what you want to edit in the prompt."
                        },
                        "aspect_ratio": {
                            "type": "string",
                            "description": "Required. Aspect ratio of the image, only these values are allowed: 1:1, 16:9, 4:3, 3:4, 9:16 Choose the best fitting aspect ratio according to the prompt. Best ratio for posters is 3:4"
                        } ,
                        "input_image": {
                            "type": "string",
                            "description": "Optional. Image to use as reference. Pass image_id here, e.g. 'im_jurheut7.png'. Best for image editing cases like: Editing specific parts of the image, Removing specific objects, Maintaining visual elements across scenes (character/object consistency), Generating new content in the style of the reference (style transfer), etc."
                        }
                    }
                },
            }
        }
    ]


async def chat_openai(messages: list, session_id: str, text_model: dict, image_model: dict, is_agent_loop_prompt = False) -> list:
    model = text_model.get('model')
    provider = text_model.get('provider')
    url = text_model.get('url')
    if provider == 'ollama' and not url.endswith('/v1'):
        # openai compatible url
        url = url.rstrip("/") + "/v1"
    
    if model is None:
        raise HTTPException(
            status_code=400,  # Bad Request
            detail="model is required"
        )
    if provider is None:
        raise HTTPException(
            status_code=400,  # Bad Request
            detail="provider is required"
        )
    if session_id is None:
        raise HTTPException(
            status_code=400,  # Bad Request
            detail="session_id is required"
        )
    await send_to_websocket(session_id, {
        'type': 'log',
        'messages': messages
    })
    payload = {
        "model": model,
        "messages": messages,
        "tools": SYSTEM_TOOLS + openai_client.tools,
        "stream": True
    }
    url = url.rstrip("/") + "/chat/completions"
    print('start chat session', model, provider, url)
    async with aiohttp.ClientSession() as session:
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {app_config.get(provider, {}).get("api_key", "")}'
        }
        async with session.post(url, json=payload, headers=headers) as response:
            combine = ''
            cur_tool_calls:list[ToolCall] = []
            content_combine = ''
            if is_agent_loop_prompt:
                messages.pop() # hide the agent loop prompt
            async for line in response.content:
                if session_id in stream_tasks and stream_tasks[session_id].cancelled():
                    print(f"ðŸ›‘Session {session_id} cancelled during stream")
                    break
                if line:
                    # Parse the JSON response
                    try:
                        # Decode bytes to string and strip whitespace
                        line_str = line.decode('utf-8').strip()
                        if not line_str:  # Skip empty lines
                            continue
                        # print('ðŸ‘‡raw line:', line_str)
                        # Handle SSE updates
                        if line_str.startswith('data: {'):
                            line_str = line_str[6:]  # Remove "data: " prefix
                            chunk = json.loads(line_str) # Parse the JSON
                            # print('ðŸ‘‡ chunk:', chunk)
                            # Extract content from the choices array
                            if 'choices' in chunk and len(chunk['choices']) > 0:
                                delta = chunk['choices'][0].get('delta', {})
                                # print('ðŸ‘‡delta', delta)
                                content = delta.get('content', '')
                                # text delta
                                if content:
                                    content_combine += content
                                    await send_to_websocket(session_id, {
                                        'type': 'delta',
                                        'text': content
                                    })
                                # tool calls
                                tool_calls = delta.get('tool_calls', [])
                                for tool_call in tool_calls:
                                    tool_call_id = tool_call.get('id')
                                    tool_call_name = tool_call.get('function', {}).get('name')
                                    
                                    if tool_call_id and tool_call_name:
                                        # tool call start
                                        
                                        tool_call_args_str = tool_call.get('function', {}).get('arguments', '')
                                        cur_tool_calls.append(ToolCall(tool_call_id, tool_call_name, tool_call_args_str))
                                        print('ðŸ¦„tool_call', tool_call_id, tool_call_name, tool_call_args_str)
                                        await send_to_websocket(session_id, {
                                            'type': 'tool_call',
                                            'id': tool_call_id,
                                            'name': tool_call_name,
                                            'arguments': tool_call_args_str if tool_call_args_str else '{}'
                                        })
                                    elif tool_call.get('function', {}).get('arguments', '') and len(cur_tool_calls) > 0:
                                        delta = tool_call.get('function', {}).get('arguments', '')
                                        cur_tool_calls[-1].arguments += delta
                                        await send_to_websocket(session_id, {
                                            'type': 'tool_call_arguments',
                                            'id': cur_tool_calls[-1].id,
                                            'text': delta # delta
                                        })
                            elif chunk.get('error'):
                                raise Exception(chunk.get('error').get('message'))

                        # Handle [DONE] marker in SSE
                        elif line_str == 'data: [DONE]':
                            continue
                        else:
                            combine += line_str
                            

                    except Exception as e:
                        traceback.print_exc()
            # print('ðŸ‘‡combine', combine)
            # print('ðŸ‘‡content_combine', content_combine)
            if content_combine != '':
                msg = {
                    'role': 'assistant',
                    'content': [{
                        'type': 'text',
                        'text': content_combine
                    }]
                }
                messages.append(msg)
                await db_service.create_message(session_id, 'assistant', json.dumps(msg))
            else:
                pass
            if len(cur_tool_calls) > 0:
                for tool_call in cur_tool_calls:
                    # append tool call to messages of assistant
                    print('ðŸ•¹ï¸tool_call', tool_call.id, tool_call.name, 'arguments', tool_call.arguments)
                    msg = {
                        'role': 'assistant',
                        'tool_calls': [{
                        'type': 'function',
                        'id': tool_call.id,
                        'function': {
                            'name': tool_call.name,
                            'arguments': tool_call.arguments if tool_call.arguments else '{}'
                            }
                        }]
                    }
                    messages.append(msg)
                    await db_service.create_message(session_id, 'assistant', json.dumps(msg))
                    # tool call args complete, execute tool call
                    model_info = {
                        'image': image_model
                    }
                    tool_result = await execute_tool(tool_call.id, tool_call.name, tool_call.arguments, session_id, model_info=model_info)
                    # append tool call result to messages of user
                    if tool_result is not None:
                        for r in tool_result:
                            messages.append(r)
                            await db_service.create_message(session_id, 'tool', json.dumps(r))
                    await send_to_websocket(session_id, {
                        'type': 'all_messages',
                        'messages': messages
                    })
            # Has Error
            if combine != '':
                    data = None
                    try:
                        data = json.loads(combine)
                    except Exception as e:
                        pass
                    if data and data.get('error') and data.get('error').get('message'):
                        # if data['error'].get('code') == 'rate_limit_error':
                        #     print('ðŸ‘‡rate_limit_error, sleeping 10 seconds')
                        #     await send_to_websocket(session_id, {
                        #         'type': 'info', 
                        #         'info': f'Hit rate limit, waiting 10 seconds before continue. {data.get("error").get("message")} Please wait for 10 seconds...'
                        #     })
                        #     await asyncio.sleep(10)
                        # else:
                        raise Exception(data.get('error').get('message'))
                    else:
                        # alert info
                        await send_to_websocket(session_id, {
                            'type': 'info', 
                            'info': combine
                        })
    return messages

def detect_image_type_from_base64(b64_data: str) -> str:
    # Only take the base64 part, not the "data:image/...," prefix
    if b64_data.startswith("data:"):
        b64_data = b64_data.split(",", 1)[1]

    # Decode just the first few bytes
    prefix_bytes = base64.b64decode(b64_data[:24])  # ~18 bytes is enough

    if prefix_bytes.startswith(b"\xff\xd8\xff"):
        return "image/jpeg"
    elif prefix_bytes.startswith(b"\x89PNG\r\n\x1a\n"):
        return "image/png"
    elif prefix_bytes.startswith(b"GIF87a") or prefix_bytes.startswith(b"GIF89a"):
        return "image/gif"
    elif prefix_bytes.startswith(b"RIFF") and b"WEBP" in prefix_bytes:
        return "image/webp"
    else:
        return "application/octet-stream"

async def execute_tool(tool_call_id: str, tool_name: str, args_str: str, session_id: str, model_info: dict = {}):
    res = []
    try:
        args_json = {}
        try:
            args_json = json.loads(args_str)
        except Exception as e:
            pass
        print('ðŸ¦„executing tool', tool_name, args_json,)
        if tool_name in SYSTEM_TOOLS_MAPPING:
            ctx = {
                'session_id': session_id,
                'model_info': model_info,
                'tool_call_id': tool_call_id,
            }
            res = await SYSTEM_TOOLS_MAPPING[tool_name](args_json, ctx)
            for r in res:
                r['tool_call_id'] = tool_call_id
            return res
        mcp_client = mcp_tool_to_server_mapping[tool_name]
        if mcp_client.session is None:
            raise Exception(f"MCP client not found for tool {tool_name}")
        result = await mcp_client.session.call_tool(tool_name, args_json)
        content_dict = [content.model_dump() for content in result.content]
        await send_to_websocket(session_id, {
            'type': 'tool_call_result',
            'id': tool_call_id,
            'content': content_dict
        })

        text_contents = [c.text if c.type == 'text' else "Image result, view user attached image below for detailed result" if c.type == 'image' else json.dumps(c.model_dump()) for c in result.content ]
        text_contents = ''.join(text_contents)
        print('ðŸ‘‡tool result text_content length', len(text_contents))
        if len(text_contents) > 10000:
            text_contents = text_contents[:10000] + "...Content truncated to 10000 characters due to length limit"

        res.append({
            'role': 'tool',
            'tool_call_id': tool_call_id,
            'content': text_contents # here only accept text string in anthropic, otherwise will throw error
        })
        for content in result.content:
            if content.type == 'image':
                image_type = detect_image_type_from_base64(content.data)
                res.append({
                    'role': 'user',
                    # 'is_tool': True,
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{image_type};base64,{content.data}",
                            },
                        },
                    ],
                })
        return res

    except Exception as e:
        print(f"Error calling tool {tool_name}: {e}")
        traceback.print_exc()
        await send_to_websocket(session_id, {
            'type': 'error',
            'error': f'Error calling tool {tool_name} with inputs {args_str[:100]} - {e}'
        })
        raise e
    return res


router = APIRouter(prefix="/api")
@router.post("/chat")
async def chat(request: Request):
    data = await request.json()
    messages = data.get('messages')
    session_id = data.get('session_id')
    canvas_id = data.get('canvas_id')
    text_model = data.get('text_model')
    image_model = data.get('image_model')
    print('ðŸ‘‡app_config.get("system_prompt", "")', app_config.get('system_prompt', ''))
    if app_config.get('system_prompt', ''):
        messages.append({
            'role': 'system',
            'content': app_config.get('system_prompt', '')
        })

    if len(messages) == 1:
        # create new session
        prompt = messages[0].get('content', '')
        # TODO: Better way to determin when to create new chat session. 
        await db_service.create_chat_session(session_id, text_model.get('model'), text_model.get('provider'), canvas_id, (prompt[:200] if isinstance(prompt, str) else ''))
    
    await db_service.create_message(session_id, messages[-1].get('role', 'user'), json.dumps(messages[-1])) if len(messages) > 0 else None

    task = asyncio.create_task(langraph_agent(messages, session_id, text_model, image_model))
    stream_tasks[session_id] = task
    try:
        await task
    except asyncio.exceptions.CancelledError:
        print(f"ðŸ›‘Session {session_id} cancelled during stream")
    finally:
        stream_tasks.pop(session_id, None)
        await send_to_websocket(session_id, {
            'type': 'done'
        })

    return {"status": "done"}

from langchain_core.messages import AIMessageChunk, ToolCall, convert_to_openai_messages, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
async def langraph_agent(messages, session_id, text_model, image_model):
        model = text_model.get('model')
        provider = text_model.get('provider')
        url = text_model.get('url')
        api_key = app_config.get(provider, {}).get("api_key", "")
        print('ðŸ‘‡model', model, provider, url, api_key)
        if provider == 'ollama':
            model = ChatOllama(
                model=model,
                base_url=url,
            )
        else:
            model = ChatOpenAI(
                model=model,
                api_key=api_key,
                timeout=1000,
                base_url=url,
                temperature=0,
                max_tokens=2048
        )
        agent = create_react_agent(
            model=model,
            tools=[generate_image_tool],
            prompt='You are a profession design agent, specializing in visual design.'
        )
        ctx = {
                'session_id': session_id,
                'model_info': {
                        'image': image_model
                    },
            }
        tool_calls: list[ToolCall] = []
        async for chunk in agent.astream(
            {"messages": messages},
            config=ctx,
            stream_mode=["updates", "messages", "custom"]
        ):
            chunk_type = chunk[0]
            
            if chunk_type == 'updates':
                all_messages = chunk[1].get('agent', chunk[1].get('tools')).get('messages', [])
                oai_messages = convert_to_openai_messages(all_messages)
                # new_message = oai_messages[-1]
                
                messages.extend(oai_messages)
                await send_to_websocket(session_id, {
                    'type': 'all_messages',
                    'messages': messages
                })
                for new_message in oai_messages:
                    await db_service.create_message(session_id, new_message.get('role', 'user'), json.dumps(new_message)) if len(messages) > 0 else None

            else:
                ai_message_chunk: AIMessageChunk = chunk[1][0]  # Access the AIMessageChunk
                content = ai_message_chunk.content  # Get the content from the AIMessageChunk
                if isinstance(ai_message_chunk, ToolMessage):
                    print('ðŸ‘‡tool_call_results', ai_message_chunk.content)
                elif content:
                    await send_to_websocket(session_id, {
                                    'type': 'delta',
                                    'text': content
                                })
                elif hasattr(ai_message_chunk, 'tool_calls') and ai_message_chunk.tool_calls and ai_message_chunk.tool_calls[0].get('name'):
                    for index, tool_call in enumerate(ai_message_chunk.tool_calls):
                        if tool_call.get('name'):
                            tool_calls.append(tool_call)
                            print('ðŸ˜˜tool_call', tool_call, tool_call.get('name'), tool_call.get('id'))
                            await send_to_websocket(session_id, {
                                'type': 'tool_call',
                                'id': tool_call.get('id'),
                                'name': tool_call.get('name'),
                                'arguments': '{}'
                            })
                elif hasattr(ai_message_chunk, 'tool_call_chunks'):
                    tool_call_chunks = ai_message_chunk.tool_call_chunks
                    for tool_call_chunk in tool_call_chunks:
                        index: int = tool_call_chunk['index']
                        if index < len(tool_calls):
                            for_tool_call: ToolCall = tool_calls[index]
                            print('ðŸ¦„sending tool_call_arguments', 'id', for_tool_call, 'text', tool_call_chunk.get('args'))
                            await send_to_websocket(session_id, {
                                'type': 'tool_call_arguments',
                                'id': for_tool_call.get('id'),
                                'text': tool_call_chunk.get('args')
                            })
                else:
                    print('ðŸ‘‡no tool_call_chunks', chunk)


@router.post("/cancel/{session_id}")
async def cancel_chat(session_id: str):
    task = stream_tasks.get(session_id)
    if task and not task.done():
        task.cancel()
        return {"status": "cancelled"}
    return {"status": "not_found_or_done"}

# @router.get("/workspace_list")
# async def workspace_list():
#     return [{"name": entry.name, "is_dir": entry.is_dir(), "path": str(entry)} for entry in Path(WORKSPACE_ROOT).iterdir()]

@router.get("/workspace_download")
async def workspace_download(path: str):
    file_path = Path(path)
    if file_path.exists() and file_path.is_file():
        return FileResponse(file_path)
    return {"error": "File not found"}

def get_ollama_model_list():
    base_url = config_service.get_config().get('ollama', {}).get('url', os.getenv('OLLAMA_HOST', 'http://localhost:11434'))
    try:
        response = requests.get(f'{base_url}/api/tags', timeout=5)
        response.raise_for_status()
        data = response.json()
        return [model['name'] for model in data.get('models', [])]
    except requests.RequestException as e:
        print(f"Error querying Ollama: {e}")
        return []

@router.get("/list_models")
async def get_models():
    config = config_service.get_config()
    res = []
    ollama_models = get_ollama_model_list()
    ollama_url = config_service.get_config().get('ollama', {}).get('url', os.getenv('OLLAMA_HOST', 'http://localhost:11434'))
    print('ðŸ‘‡ollama_models', ollama_models)
    for ollama_model in ollama_models:
        res.append({
            'provider': 'ollama',
            'model': ollama_model,
            'url': ollama_url,
            'type': 'text'
        })
    for provider in config.keys():
        models = config[provider].get('models', {})
        for model_name in models:
            if provider == 'ollama':
                continue
            if provider != 'comfyui' and config[provider].get('api_key', '') == '':
                continue
            model = models[model_name]
            res.append({
                'provider': provider,
                'model': model_name,
                'url': config[provider].get('url', ''),
                'type': model.get('type', 'text')
            })
    return res

mcp_clients: dict[str, MCPClient] = {}
mcp_clients_status = {}
mcp_tool_to_server_mapping: dict[str, MCPClient] = {}

async def initialize():
    # await initialize_mcp()
    for session_id in active_websockets:
        await send_to_websocket(session_id, {
            'type': 'init_done'
        })

async def initialize_mcp():
    print('ðŸ‘‡initializing mcp')
    mcp_config_path = os.path.join(USER_DATA_DIR, "mcp.json")
    if not os.path.exists(mcp_config_path):
        return {}
    with open(mcp_config_path, "r") as f:
        json_data = json.load(f)
    global mcp_clients
    global mcp_clients_status
    global mcp_tool_to_server_mapping
    mcp_clients_json: dict[str, dict] = json_data.get('mcpServers', {})
    
    for server_name, server in list(mcp_clients_json.items()):
        if server.get('command') is None:
            continue
        if server.get('args') is None:
            server['args'] = []
        
        mcp_client = MCPClient()
        mcp_clients_status[server_name] = {
            'status': 'initializing'
        }
        try:
            await mcp_client.connect_to_server(server["command"], server["args"], server.get('env'))
            mcp_clients_status[server_name] = {
                'status': 'connected',
                'tools': mcp_client.tools
            }
            print('ðŸ‘‡mcp_client connected', server_name, 'tools', len(mcp_client.tools))
            for tool in mcp_client.tools:
                mcp_tool_to_server_mapping[tool['name']] = mcp_client
                print('tool', tool)
                openai_client.tools.append({
                    'type': 'function',
                    'function': {
                        'name': tool['name'],
                        'description': tool['description'],
                        'parameters': tool['input_schema']
                    }
                })
                anthropic_client.tools.append(tool)
                ollama_client.tools.append({
                    'type': 'function',
                    'function': {
                        'name': tool['name'],
                        'description': tool['description'],
                        'parameters': tool['input_schema']
                    }
                })

            mcp_clients[server_name] = mcp_client
        except Exception as e:
            print(f"Error connecting to MCP server {server_name}: {e}")
            traceback.print_exc()
            mcp_clients_status[server_name] = {
                'status': 'error',
                'error': str(e)
            }

@router.get("/list_mcp_servers")
async def list_mcp_servers():
    if mcp_clients is None:
        return {}
    mcp_config_path = os.path.join(USER_DATA_DIR, "mcp.json")
    if not os.path.exists(mcp_config_path):
        return {}
    with open(mcp_config_path, "r") as f:
        json_data = json.load(f)
    mcp_servers = json_data.get('mcpServers', {})
    for server_name, server in mcp_servers.items():
        mcp_servers[server_name]['tools'] = mcp_clients_status[server_name].get('tools', [])
        mcp_servers[server_name]['status'] = mcp_clients_status[server_name].get('status', 'error')
    return mcp_servers

@router.get("/list_chat_sessions")
async def list_chat_sessions():
    return await db_service.list_sessions()

@router.get("/chat_session/{session_id}")
async def get_chat_session(session_id: str):
    return await db_service.get_chat_history(session_id)

